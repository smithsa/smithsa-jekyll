<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-06-30T10:53:16-05:00</updated><id>http://localhost:4000/</id><title type="html">Sade Smith</title><subtitle>Software Developer, with 5 years of experience, currently working in the Chicagoland area, specializing in web technologies and skilled in both back-end and front-end development.</subtitle><entry><title type="html">Scraping Client Side Rendered Data with Python and Selenium</title><link href="http://localhost:4000/scraping-client-side-rendered-data-with-python-and-selenium" rel="alternate" type="text/html" title="Scraping Client Side Rendered Data with Python and Selenium" /><published>2018-06-15T00:00:00-05:00</published><updated>2018-06-15T00:00:00-05:00</updated><id>http://localhost:4000/scraping-client-side-rendered-data-with-python-and-selenium</id><content type="html" xml:base="http://localhost:4000/scraping-client-side-rendered-data-with-python-and-selenium">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Scraping data from websites often offers a way to automate tedious tasks and improve productivity. However, many people run into issues when the content of a website is generated on the client side as opposed to the server-side. For instance, content can not be retreived with just a HTTP request for websites that utilize AJAX to generate it’s content. A web scrapper using only server-side requests would be unable to scrape the data of such a site because the HTML of the page does not load until the javascript of the site can be executed. Since the Javascript can only be executed on the client side, a request from the page would not include it’s dynamic content.&lt;/p&gt;

&lt;p&gt;To solve this problem, you can use a browser automation tool such as Selenium or PhantomJs in combination with your web scraper script. Using a browser automation tool, the HTML is able to be generated and thus read and parsed. In this tutorial, I will be showing you how to write a basic script to scrape client-side rendered content with Python and Selenium.&lt;/p&gt;

&lt;p&gt;Remember before you scrape content or data from websites, ensure that you have legal rights to do so, read the site’s terms of service agreement to see if scraping is allowed, and ensure that an alternative method for to retrieve the data does not exist, for example an API. In addition, check the robots.txt file and follow the rules for how frequently you are allowed to request pages and what pages are allowed to be scraped.&lt;/p&gt;

&lt;p&gt;The Python Selenium library includes a HTML tree parser but we will use Selenium to load the page’s content and BeautifulSoup to parse the HTML, since BeautifulSoup is Python’s most popular parser.&lt;/p&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Python 2.7 programming environment available here (&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;https://www.python.org/downloads/&lt;/a&gt;){:target=”_blank”}&lt;/li&gt;
  &lt;li&gt;MacOS or Linux Environment&lt;/li&gt;
  &lt;li&gt;Virtualenv available here (&lt;a href=&quot;https://pypi.org/project/virtualenv/&quot;&gt;https://pypi.org/project/virtualenv/&lt;/a&gt;){:target=”_blank”}&lt;/li&gt;
  &lt;li&gt;pip, package manager for available here python (&lt;a href=&quot;https://pip.pypa.io/en/stable/installing/&quot;&gt;https://pip.pypa.io/en/stable/installing/&lt;/a&gt;){:target=”_blank”}&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;virtual-environment-setup&quot;&gt;Virtual Environment Setup&lt;/h2&gt;

&lt;p&gt;Before we start writing the script, we will first set up a virtual environment. Let’s first create a directory for the project called simple-scrapper and navigate to the directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    mkdir simple-scrapper
    cd simple-scrapper
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can create the virtual environment and activate the environment with the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    virtualenv venv
    source venv/bin/activate
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;installing-python-libraries&quot;&gt;Installing Python Libraries&lt;/h2&gt;

&lt;p&gt;Next, we can install all the python modules and libraries we will need for the script with pip. We will need the selenium python bindings, BeatifulSoup4, and the requests library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    pip install selenium
    pip install BeautifulSoup4
    pip install requests
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;downloading-selenium-driver&quot;&gt;Downloading Selenium Driver&lt;/h2&gt;

&lt;p&gt;Now that we have all our libraries installed, we should download a driver for Selenium. The driver is required for Selenium to interface the browser you choose to use. For our purposes, we will use Chrome as our browser. You can download the latest release of the Chrome driver here: http://chromedriver.chromium.org/downloads. Download the appropriate zip file for your operating system, unzip the file, and you should find an executable file named “chromedriver.” Put this file at the root of the “simple-scrapper” folder we just created.&lt;/p&gt;

&lt;h2 id=&quot;the-web-scraping-script&quot;&gt;The Web Scraping Script&lt;/h2&gt;

&lt;p&gt;We are now ready to start writing the scraper. Let’s create a python file in at the root of the folder “simple-scraper” called “scrapper.py.”&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    touch scrapper.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open the file with your preferred text editor and let’s start writing the code to scrape a web page. For our purposes, I have created a basic page to scrape that has client-side rendered HTML. You can find the example file here: &lt;a href=&quot;http://sadesmith.com/examples/simple-scrapper/index.html&quot; target=&quot;_blank&quot;&gt;http://sadesmith.com/examples/simple-scrapper/index.html&lt;/a&gt;. Lets quickly examine the content of the page. The page simply lists the names of basketball players on the 2018 Golden State Warriors Roster — there is one header and an unordered list. Our goal will be simple, scrape all the names of the roster and print them out once collected.&lt;/p&gt;

&lt;p&gt;The first task will be to load the webpage using the driver we just downloaded. We will need to tell the WebDriver where the driver is located by setting the executable path on the Chrome class. Then we can request the webpage by using the get method of the driver.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    import os
    from selenium import webdriver

    driver = webdriver.Chrome(executable_path= os.path.abspath('')+'/chromedriver')
    url = &quot;http://www.sadesmith.com/examples/simple-scrapper/index.html&quot;
    driver.get(url)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we will need to tell the WebDriver to wait a few seconds before parsing the page. We will import the time module and use the sleep method. We will give the method the value of 3, thus the wait time will be 3 seconds.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    import os
    from selenium import webdriver
    import time

    driver = webdriver.Chrome(executable_path= os.path.abspath('')+'/chromedriver')
    url = &quot;http://www.sadesmith.com/examples/simple-scrapper/index.html&quot;
    driver.get(url)
    time.sleep(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are ready to parse but before we do that let’s import beautiful soup.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    from bs4 import BeautifulSoup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can grab the content we need to parse. With the WebDriver loading the page, we can now view content loaded client-side. WebDriver’s page_source will return the source code of the DOM as a string. We can use BeautifulSoup to parse the HTML string, with its HTML parser.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    page_html = driver.page_source
    bsoup = BeautifulSoup(page_html, 'html.parser')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the BeautifulSoup object, we can then use the library to its full extent. If you are unfamiliar with BeautifulSoup you can find more documentation here about how to parse: https://www.crummy.com/software/BeautifulSoup/bs4/doc/. For our example, we know that all the names on the Warrior’s rosters are in an unordered list, in list item tag with the class name of “name”. So we can grab all list item elements with the class attribute name then loop through them print them out.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    player_names = bsoup.findAll('li', attrs={'class': 'name'});
    for player_name in player_names:
      print player_name.text
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;wrapping-it-all-up&quot;&gt;Wrapping it all up&lt;/h2&gt;

&lt;p&gt;We are done with the simple scrapper and your file should now look this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    import os
    from selenium import webdriver
    import time
    from bs4 import BeautifulSoup

    driver = webdriver.Chrome(executable_path= os.path.abspath('')+'/chromedriver')
    driver.get(url)
    time.sleep(3)

    page_html = driver.page_source
    bsoup = BeautifulSoup(page_html, 'html.parser')

    player_names = bsoup.findAll('li', attrs={'class': 'name'});
    for player_name in player_names:
        print player_name.text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s run the project in the terminal! Make sure your current working directory is “simple-scrapper” and run the script.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    python scrapper.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If everything went well, you should see the names of the players on the 2018 Golden State Warriors printed in your terminal screen. We have accomplished our objective! Via our simple web scraper script, we were able to scrape HTML that was generated on the client-side using Selenium and Python.&lt;/p&gt;</content><author><name></name></author><category term="python" /><category term="selenium" /><category term="automation" /><category term="tutorial" /><category term="web scraping" /><summary type="html">Scraping data from websites often offers a way to automate tedious tasks and improve productivity. However, many people run into issues when the content of a website is generated on the client side as opposed to the server-side.</summary></entry><entry><title type="html">Overview of the HTTP/2 Protocol</title><link href="http://localhost:4000/overview-of-the-http2-protocol" rel="alternate" type="text/html" title="Overview of the HTTP/2 Protocol" /><published>2018-06-02T00:00:00-05:00</published><updated>2018-06-02T00:00:00-05:00</updated><id>http://localhost:4000/overview-of-the-http2-protocol</id><content type="html" xml:base="http://localhost:4000/overview-of-the-http2-protocol">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2015, the HyperText Transfer Protocol (HTTP) underwent a major revision when HTTP/2 became officially standardized. With the protocol’s last major update being in 1999 (HTTP/1.1), it was a much needed upgrade. The previous version of the protocol, HTTP/1.1, proved to posses numerous limitations for most modern websites. The average HTTP requests per page have risen dramatically overtime as well as page size. Simply put, HTTP/1.1 was developed to handle websites of its time and not of today. These limitations later led developers to use HTTP/1.1 in ways it was not meant to be used, causing developers to create workarounds which developed into best practices. For example, with HTTP/1.1 a client only has a limited number of TCP connections (6 connections) to a server, and each request requires one of those connections. With limited connections a request must wait for another to finish before being processed (HOL Blocking), resulting in a bottleneck for websites retrieving resources, slowing down performance. To solve this problem, file concatenation, image sprites, domain sharding, using gzip, and resource inlining became workarounds and best practices for this particular issue of HOL Blocking. These solutions proved to be hacks, not really solving the issue at hand, which was really at the protocol level.&lt;/p&gt;

&lt;p&gt;In 2009, Google recognized these issues for their own operations of transporting web content and began working on an internal project called SPDY. SPDY was a network protocol that offered solutions to many of HTTP/1.1’s shortcomings, ofadfering addresings issues o web security and improved page load latency. This protocol would later become the foundation of HTTP/2.&lt;/p&gt;

&lt;p&gt;Fast forward to 2018, SPDY is deprecated and HTTP/2 is the new current version of HTTP with adoption increasing. HTTP/2 resolves the problems with HTTP/1.1 at the protocol level and provides faster performance for a website. The new protocol is backwards compatible and has the same semantics as HTTP/2, but improves how data is sent and what data is sent. The core features of HTTP/2 are multiplexing, binary framing, header compression, prioritization, and server push. With the update, many of the best practices developers utilize are invalidated and are detrimental to performance. These new core features of HTTP/2 will require new best practices for developers to utilize it benefits.&lt;/p&gt;

&lt;h2 id=&quot;multiplexingbinary-framingstream-prioritization&quot;&gt;Multiplexing/Binary Framing/Stream Prioritization&lt;/h2&gt;

&lt;p&gt;First, let’s talk about multiplexing and binary framing. HTTP/2 now technically utilizes a single TLS encrypted connection. While HTTP/1.1 uses a request/response pair, HTTP/2 uses a logical stream. These streams are put into binary frames, encoded in a binary format, and then put on the connection. Streams share the connection and the bandwidth of the connection (multiplexing). If one stream is in a blocking state, this does not prevent other streams from accessing the connection and it’s resources. Additionally, a bonus, the client can specify dependencies and weight for each stream, this is called stream prioritization. But now with HTTP/2 multiple requests for data can be sent in parallel - multiple requests at the same time. HOL blocking is no longer a problem, where it was in HTTP/1.1. Now because requests are cheap, best practices for receiving more out of a request and reduce the number of requests, such as concatenation and spriting, no longer apply.&lt;/p&gt;

&lt;h2 id=&quot;header-compression&quot;&gt;Header Compression&lt;/h2&gt;

&lt;p&gt;Another major issue with HTTP/1.1 pertained to the metadata sent in header. As a result of HTTP being stateless, redundant, long, and static information was sent across each request. One such examples is cookies and user agent data. In addition, the data sent in the headers was in human readable text. These two things combined created the issue of wasted of bandwidth. Developers tried to work around this issue by using gzip to compress the data that goes over the connection. But gzip only compresses the data and not the headers itself. Now with HTTP/2, headers can be compressed because they are seperated from the data. HTTP/2 compresses headers using a new technology specifically for HTTP header compression called HPACK. However with the use of HPACK, compression and decompression works on the connection, meaning more connections are counterproductive. Best practices such as domain sharding and use of multiple origins for cdns are no longer considered best practice for HTTP/2.&lt;/p&gt;

&lt;h2 id=&quot;server-push&quot;&gt;Server Push&lt;/h2&gt;

&lt;p&gt;Lastly, let’s discuss the new push feature. With HTTP/2, the server can respond to a request that hasn’t been sent yet. With this new feature, the server can send any asset or file to the client it thinks it will need. For example, the server can send the css and javascript file because it understands that the client will need it as well after an intial request from the browser is sent for the index.html. Once the client is ready to recieve those files pushed, it will be ready for the client waiting in the cache. Adding further additional optimization of speed.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;To take advantage of the new protocol, there are some new best practices to follow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reduce DNS Lookups&lt;/li&gt;
  &lt;li&gt;Reuse TCP connections, connections are expensive&lt;/li&gt;
  &lt;li&gt;Use a CDN but reduce the number of origins&lt;/li&gt;
  &lt;li&gt;Minimize number of HTTP redirects&lt;/li&gt;
  &lt;li&gt;Eliminate unnecessary metadata&lt;/li&gt;
  &lt;li&gt;Compress assets during transfer (let gzip compress on the server)&lt;/li&gt;
  &lt;li&gt;Cache resources on the client&lt;/li&gt;
  &lt;li&gt;Eliminate unnecessary resources. Fetch what you need. Bytes are expensive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As stated previously, since 2015, the world wide web has seen increasing adoption of the new protocol. Companies such as Cloudflare, major hosting providers like Bluehost, and most majors browsers now offer support for the technology. If your site is using a TLS/SSL connection, and your servers from your hosting company support HTTP/2, it is likely that you are already using HTTP/2! Every connection starts out as HTP/1.1 then upgrades to HTTP/2 if all requirements are met. These three requirements must be met to use HTTP/2: (1) Server must support HTTP/2, (2) your application must use an HTTPS connection via TLS/SSL, and (3) the browser of a user must support HTTP/2. If at least one of the requirements are not met, then HTTP/1.1 is used. Developers should be looking to take advantage of the new upgrades provided by HTTP/2 as it’s adoption continues to grow. It’s essentially free optimization for your website!&lt;/p&gt;</content><author><name></name></author><category term="http" /><category term="http/2" /><category term="http/1.1" /><category term="spdy" /><summary type="html">In 2015, the HyperText Transfer Protocol (HTTP) underwent a major revision when HTTP/2 became officially standardized. With the protocol's last major update being in 1999 (HTTP/1.1), it was a much needed upgrade. The previous version of the protocol, HTTP/1.1, proved to posses numerous limitations for most modern websites.</summary></entry></feed>