<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-08-07T10:34:39-05:00</updated><id>http://localhost:4000/</id><title type="html">Sade Smith</title><subtitle>Software Developer, with 5 years of experience, currently working in the Chicagoland area, specializing in web technologies and skilled in both back-end and front-end development.</subtitle><entry><title type="html">10 Tips for Web Accessibility (Part 1)</title><link href="http://localhost:4000/2018/08/05/blog/10-tips-for-web-accessiblity-part-1/" rel="alternate" type="text/html" title="10 Tips for Web Accessibility (Part 1)" /><published>2018-08-05T00:00:00-05:00</published><updated>2018-08-05T00:00:00-05:00</updated><id>http://localhost:4000/2018/08/05/blog/10-tips-for-web-accessiblity-part-1</id><content type="html" xml:base="http://localhost:4000/2018/08/05/blog/10-tips-for-web-accessiblity-part-1/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The benefits of developing accessible websites undoubtedly weight the costs. First and foremost, all potential users are able to use your web application. Users with disabilities will be able to access information and perform tasks just as those without disabilities. Secondly, accessibility (a11y) best practices share similar practices with other web related best practices such as search engine optimization (SEO) and usability standards. Following the best practices for accessibility means you will be following the best practices of the web. In this two-part series, I will be giving some practical development tips based on recommendations from the Web Content Accessibility Guidelines (WCAG) that you can use to develop more accessible websites.&lt;/p&gt;

&lt;h2 id=&quot;1-use-native-html-elements-properly&quot;&gt;1. Use Native HTML Elements Properly&lt;/h2&gt;

&lt;p&gt;It is important that you use native HTML elements because they have accessibility benefits built-in. For example, using the button tag allows a button to be automatically indexed with the tab button and activated using the keyboard. If you were to not use the button tag and create your own button with a custom tag or div these benefits would not be afforded to you. Don’t recreate the wheel unless you have an extremely good reason. If you decide to create your own custom elements be sure to create keyboard accessibility and provide a tabindex attribute of 0.&lt;/p&gt;

&lt;p&gt;In addition, your HTML should be written in a way that conveys the semantics of a given page. Good semantics help assistive technology devices, such as screen readers, perform their jobs by giving them a map of what is being parsed. You should consider using HTML5 semantic elements like header, nav, footer, aside, article, and section to structure your pages. Also, strive to use and structure these HTML elements correctly. Inconsistencies or improper structuring of your HTML can confuse and throw screen readers off.&lt;/p&gt;

&lt;h2 id=&quot;2-use-appropriate-html-heading-tags&quot;&gt;2. Use Appropriate HTML Heading Tags&lt;/h2&gt;
&lt;p&gt;Along similar lines of the tip #1, you should use the appropriate HTML heading tag and ensure the correct order. With &lt;code&gt;h1&lt;/code&gt; being the highest section level heading and &lt;code&gt;h6&lt;/code&gt; being the lowest. In the scenarios that your designs do not contain a heading, you can add a header that describes the sections succinctly and then style the heading so that it does not visually appear on the screen. Adding heading tags allows assistive technology devices to understand what is on the page and are good for but also SEO. I like to use a special class for these instances (see example below).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;.a11y-hidden{
	position: absolute;
	z-index: -9999;
	width:1px;
	height: 1px;
	font-size: 0;
	border: 0;
	padding: 0;
	margin: 0;
	overflow: hidden;
	white-space: nowrap;
	clip: rect(0,0,0);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;3-use-background-images-for-decorative-images&quot;&gt;3. Use Background Images for Decorative Images&lt;/h2&gt;
&lt;p&gt;Some designs have images that don’t convey any information to the user and are there for style or decorative purposes. These images should be displayed using the CSS &lt;code&gt;background-image&lt;/code&gt; property if possible. If you determine that an image tag is needed, then make sure that the &lt;code&gt;alt&lt;/code&gt;  attribute is empty so that screen readers can skip over reading the image.&lt;/p&gt;

&lt;h2 id=&quot;4-dont-remove-css-outline&quot;&gt;4. Don’t Remove CSS Outline&lt;/h2&gt;
&lt;p&gt;There are times when you create a link or a button and see a blue glowing outline on your element. Sometimes it clashes with the overall design, so to remove it developers change the outline CSS property to none. It’s a big accessibility no-no. It makes it difficult for those who do not use a mouse to see where they are currently at on a page if they are for instance tabbing through. In the cases, that you do decide that you can’t have that blue glow, go ahead and use &lt;code&gt;outline: none&lt;/code&gt; in your CSS but also style the focused (:focus) state of the element in a way that clearly shows that it is selected. It is a good practice to style your focus states the same as your hover state.&lt;/p&gt;

&lt;h2 id=&quot;5-always-use-a-meta-viewport-tag&quot;&gt;5. Always Use a Meta Viewport Tag&lt;/h2&gt;
&lt;p&gt;Using a meta viewport tag is probably the easiest tip in this post to implement. If your website is missing this tag, mobile devices will render the pages at desktop widths, then scale the page for the mobile device. So the meta viewport tag will ensure that the page fits the width of the device. Google recommends using the following meta tag in the head of your pages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;head&amp;gt;
  ...
  &amp;lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&amp;gt;
  ...
&amp;lt;/head&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;wrapping-it-up&quot;&gt;Wrapping It Up&lt;/h2&gt;
&lt;p&gt;As you can see, these are very simple tips to implement in your website to make them more accessible and provide an overall improvement. The best part, there’s More! Stay tuned for part 2, where we will continue going over practical tips for developing accessible websites. Getting into more accessibility principles, I will be touching upon ARIA, skip links, form labels, and font sizes.&lt;/p&gt;</content><author><name></name></author><category term="accessibility" /><category term="a11y" /><category term="wcag" /><summary type="html">Following the best practices for accessibility means you will be adhering to the best practices of the web. In this two-part series, I will be giving some practical development tips based on recommendations from the Web Content Accessibility Guidelines (WCAG) that you can use to develop more accessible websites.</summary></entry><entry><title type="html">3 Browser APIs to be Excited About</title><link href="http://localhost:4000/2018/07/18/blog/3-browser-apis-to-be-excited-about/" rel="alternate" type="text/html" title="3 Browser APIs to be Excited About" /><published>2018-07-18T00:00:00-05:00</published><updated>2018-07-18T00:00:00-05:00</updated><id>http://localhost:4000/2018/07/18/blog/3-browser-apis-to-be-excited-about</id><content type="html" xml:base="http://localhost:4000/2018/07/18/blog/3-browser-apis-to-be-excited-about/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Each year web technologies push the boundaries of what was thought possible. The gap between native and web applications steadily narrows as browsers introduce new web/browser application programming interfaces (APIs). Evidence of this is seen in the standardization of web/browser APIs like Geolocation and AmbientLightSensor. These APIs remove capability barriers by providing access to device hardware. Additionally, we can see APIs like Web Storage and IndexedDB adding a performance boost by storing data on the client-side. The best part is that the list of browser APIs keeps growing, and in turn, enhancing the capability of the web. It’s certainly an exciting time to be developing for the web!&lt;/p&gt;

&lt;p&gt;In this post, we will be exploring some of these APIs improving the everyday web experience. I will be giving an overview of five exciting browser APIs you should know about. For each API, we will go over what the API does, potential use cases, and current browser support. Developers should consider implementing these new technologies in their web applications once browsers adopt and ship them. Until then, be sure to feature detect if you decide to use these APIs sooner than later.&lt;/p&gt;

&lt;h2 id=&quot;webshare-api&quot;&gt;Webshare API&lt;/h2&gt;
&lt;p&gt;Currently, there isn’t a standard way to access the share dialog of native devices using the web. The &lt;a href=&quot;https://developers.google.com/web/updates/2016/09/navigator-share&quot;&gt;WebShare API&lt;/a&gt; fixes this problem and allows your web applications to invoke native device’s share dialog. Developers can access the API through the navigator object, calling the share() method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;    navigator.share()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It makes sharing easier for users and gives them more control over how they would like to share information from the web. Potential use cases for the Webshare API are for sites that currently use share buttons to share content (i.e.: blogs and sites that have a lot of short-form media like images). If you want to use this API, simply provide the share method with an object that has the following properties listed below in the code example. At a minimum, the properties of text and URL are required. Another requirement is that the website you share must use HTTPS. Additionally, the Web Share API is promised-based so you can use promises and catch errors with this method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;	navigator.share({
		title: ‘Sade Smith’,
		text: ’3 Browser APIs you should be excited about’,
		url: '[https://sadesmith.com](https://sadesmith.com/),
	})
	.then(() =&amp;gt; console.log('Successful share'))
	.catch((error) =&amp;gt; console.log('Error sharing', error
	));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below you can see the functionality of the API in action courtesy of &lt;a href=&quot;[https://paul.kinlan.me/](https://paul.kinlan.me/)&quot;&gt;Paul Kinlan&lt;/a&gt; from Google.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lhUzYxCvWew&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;small&gt;Payment Request API, courtesy of Google Chrome Developers&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;I know what you are thinking about: browser support, right? The only browsers that offer full support are Chrome 61 android and Opera 48 and Webkit is considering adding support as well. It will be interesting to see if other browsers move to adopt this API. If there is a wider adoption, then that would mean the web would be a step closer to closing the gap between the web and native. For now, if you decide to use this API, you should feature-detect to ensure that the API is available on the user’s platform.strong text&lt;/p&gt;

&lt;h2 id=&quot;payment-request-api&quot;&gt;Payment Request API&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://developers.google.com/web/fundamentals/payments/&quot;&gt;Payment Request API&lt;/a&gt; aims to make the traditional tedious checkout process on website obsolete by storing users billing information in the browser and presenting a consistent, secure and user-friendly interface regardless of payment method. Checkout forms are notoriously long and repetitive across sites and filling out these forms on mobile devices are even more of a headache. This API streamline the checkout process and makes the checkout flow a lot shorter. Now with the Payment Request API, you can checkout with a few click and keyboard is not needed at all. Amazing, right? So let’s talk about how it works.&lt;/p&gt;

&lt;p&gt;The browser will securely store and save your information for purchases and then send it to the merchant. (the address in this) The merchant then can send back options for shipping. The only required input from the user would is the CVC. Once the user confirms the payment, the data is securely bundled and sent to the merchant. Once the browser determines that the accepted payment methods for the site and the methods on the device are compatible, the browser will display the payments UI. The user then selects the payment method and authorized the transaction. The data the user entered and has stored (i.e. card number, CVC, card expiration date) is bundled and sent to the merchant site.&lt;/p&gt;

&lt;p&gt;It’s important to note that this is not a new payment method itself. It does not replace payment processors like Stripe and Braintree. Nor does it interact directly with payment processors. The API is simply a way to send user’s payment and shipping information to merchants to allow for a seamless and painless checkout experience on any browser, device, or platform. So it’s vendor agnostic, it does not matter what payment processing service you use.&lt;/p&gt;

&lt;p&gt;The use cases for this API are any websites that sell services or products. E-ccommerce sites can definitely use this API to their advantage. The main benefit is that it can increase conversion rates for online businesses by decreasing the drop-off that happens during the checkout process. The API removes the friction involved in making a purchase, thus making it more likely a user will complete the process. If you choose to use this API, your website must be served over a secure connection (use HTTPS).&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/hmqZxP6iTpo&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;small&gt;Payment Request API, courtesy of Google Chrome Developers&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The Payment Request API is currently supported in Chrome, Safari, and Edge. Additionally, Firefox is working on an implementation. It is a W3C standard candidate so hopefully, we see it being adopted by more browser engines in the future.&lt;/p&gt;

&lt;h2 id=&quot;service-worker-api&quot;&gt;Service Worker API&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://developers.google.com/web/ilt/pwa/introduction-to-service-worker&quot;&gt;Service workers&lt;/a&gt; are web workers that run in the background of your browser, separate from the main thread, running asynchronously. It is independent of the website itself so it does not have access to the DOM. Service workers are essentially a JavaScript file that has control of the associated website’s network requests and cache resources. In addition, the service worker can receive push messages for a server when the web application is not open so notifications can be pushed to the user without the website being opened. They can be thought of as a proxy server that positioned between a web application, the browser, and the network (when a network can be accessible).&lt;/p&gt;

&lt;p&gt;The Service Worker API is immensely important to the web because it allows for devices to access content when they are offline or have limited connectivity and allows websites to send push notifications even when the site is not open. These two features were previously only capable in native applications. Service workers combined with other APIs help the web get closer to native mobile apps by providing features native apps previously only held. Being able to access content from the web offline or with a poor connection is big!&lt;/p&gt;

&lt;p&gt;Using a service worker, requires you to register it in your website’s Javascript and serve your application over a secure connection. Once the service worker is registered, the browser will start the installation step in the background. The next step is activation, where the service worker can finish setup or clean related resources (for example, cleaning older caches). You can find more information on how to use the Service Worker API from &lt;a href=&quot;[https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers](https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers)&quot;&gt;Mozilla’s service worker documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The most obvious use case would be to use this API for offline experiences. The Service Worker API is being used in progressive web applications (PWAs), which are regular websites which appear to have a native feel and offer an immersive experience like native apps. Service workers allow these web applications to run offline, a feature only mobile applications used to have. As a result, PWAs are indistinguishable from native mobile applications to the average user.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/U35B31dBvBk&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;small&gt;Demo, courtesy of &lt;a href=&quot;https://github.com/davidnguyen179&quot;&gt;Dung Nguyen&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;There is good news in terms of browser support for service workers! Most of the major browsers support service workers, that includes Edge, Chrome, Safari, and Firefox. You can find more detailed information at &lt;a href=&quot;[https://jakearchibald.com/](https://jakearchibald.com/)&quot;&gt;Jake Archibald’s&lt;/a&gt; site named &lt;a href=&quot;[https://jakearchibald.github.io/isserviceworkerready/](https://jakearchibald.github.io/isserviceworkerready/)&quot;&gt;“Is Service Worker Ready”&lt;/a&gt; but it is definitely a good idea to start using the API now&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With these new browser APIs, there has never been a better time to be developing for the web in my opinion. The list could continue: Web Bluetooth, WebVR, Shape Detection, WebAssembly, and Credential Management are APIs to keep your eye on as well. Without a doubt, the web as a platform remains a good bet and is progressively growing stronger. These APIs are worth everyone’s attention.&lt;/p&gt;</content><author><name></name></author><category term="javascript" /><category term="browser" /><category term="web share" /><category term="web bluetooth" /><category term="service workers" /><category term="payment requests" /><category term="pwas" /><category term="apis" /><summary type="html">The gap between native and web applications steadily narrows as browsers introduce new web/browser application programming interfaces (APIs). Evidence of this is seen in the standardization of web/browser APIs like Geolocation and AmbientLightSensor. These APIs remove capability barriers by providing access to device hardware.</summary></entry><entry><title type="html">Scraping Client Side Rendered Data with Python and Selenium</title><link href="http://localhost:4000/2018/06/15/blog/scraping-client-side-rendered-data-with-python-and-selenium/" rel="alternate" type="text/html" title="Scraping Client Side Rendered Data with Python and Selenium" /><published>2018-06-15T00:00:00-05:00</published><updated>2018-06-15T00:00:00-05:00</updated><id>http://localhost:4000/2018/06/15/blog/scraping-client-side-rendered-data-with-python-and-selenium</id><content type="html" xml:base="http://localhost:4000/2018/06/15/blog/scraping-client-side-rendered-data-with-python-and-selenium/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Scraping data from websites often offers a way to automate tedious tasks and improve productivity. However, many people run into issues when the content of a website is generated on the client side as opposed to the server-side. For instance, content can not be retreived with just a HTTP request for websites that utilize AJAX to generate it’s content. A web scrapper using only server-side requests would be unable to scrape the data of such a site because the HTML of the page does not load until the javascript of the site can be executed. Since the Javascript can only be executed on the client side, a request from the page would not include it’s dynamic content.&lt;/p&gt;

&lt;p&gt;To solve this problem, you can use a browser automation tool such as Selenium or PhantomJs in combination with your web scraper script. Using a browser automation tool, the HTML is able to be generated and thus read and parsed. In this tutorial, I will be showing you how to write a basic script to scrape client-side rendered content with Python and Selenium.&lt;/p&gt;

&lt;p&gt;Remember before you scrape content or data from websites, ensure that you have legal rights to do so, read the site’s terms of service agreement to see if scraping is allowed, and ensure that an alternative method for to retrieve the data does not exist, for example an API. In addition, check the robots.txt file and follow the rules for how frequently you are allowed to request pages and what pages are allowed to be scraped.&lt;/p&gt;

&lt;p&gt;The Python Selenium library includes a HTML tree parser but we will use Selenium to load the page’s content and BeautifulSoup to parse the HTML, since BeautifulSoup is Python’s most popular parser.&lt;/p&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Python 2.7 programming environment available here (&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;https://www.python.org/downloads/&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;MacOS or Linux Environment&lt;/li&gt;
  &lt;li&gt;Virtualenv available here (&lt;a href=&quot;https://pypi.org/project/virtualenv/&quot;&gt;https://pypi.org/project/virtualenv/&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;pip, package manager for available here python (&lt;a href=&quot;https://pip.pypa.io/en/stable/installing/&quot;&gt;https://pip.pypa.io/en/stable/installing/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;virtual-environment-setup&quot;&gt;Virtual Environment Setup&lt;/h2&gt;

&lt;p&gt;Before we start writing the script, we will first set up a virtual environment. Let’s first create a directory for the project called simple-scrapper and navigate to the directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    mkdir simple-scrapper
    cd simple-scrapper
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can create the virtual environment and activate the environment with the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    virtualenv venv
    source venv/bin/activate
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;installing-python-libraries&quot;&gt;Installing Python Libraries&lt;/h2&gt;

&lt;p&gt;Next, we can install all the python modules and libraries we will need for the script with pip. We will need the selenium python bindings, BeatifulSoup4, and the requests library.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    pip install selenium
    pip install BeautifulSoup4
    pip install requests
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;downloading-selenium-driver&quot;&gt;Downloading Selenium Driver&lt;/h2&gt;

&lt;p&gt;Now that we have all our libraries installed, we should download a driver for Selenium. The driver is required for Selenium to interface the browser you choose to use. For our purposes, we will use Chrome as our browser. You can download the latest release of the Chrome driver here: http://chromedriver.chromium.org/downloads. Download the appropriate zip file for your operating system, unzip the file, and you should find an executable file named “chromedriver.” Put this file at the root of the “simple-scrapper” folder we just created.&lt;/p&gt;

&lt;h2 id=&quot;the-web-scraping-script&quot;&gt;The Web Scraping Script&lt;/h2&gt;

&lt;p&gt;We are now ready to start writing the scraper. Let’s create a python file in at the root of the folder “simple-scraper” called “scrapper.py.”&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    touch scrapper.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open the file with your preferred text editor and let’s start writing the code to scrape a web page. For our purposes, I have created a basic page to scrape that has client-side rendered HTML. You can find the example file here: &lt;a href=&quot;http://sadesmith.com/examples/simple-scrapper/index.html&quot; target=&quot;_blank&quot;&gt;http://sadesmith.com/examples/simple-scrapper/index.html&lt;/a&gt;. Lets quickly examine the content of the page. The page simply lists the names of basketball players on the 2018 Golden State Warriors Roster — there is one header and an unordered list. Our goal will be simple, scrape all the names of the roster and print them out once collected.&lt;/p&gt;

&lt;p&gt;The first task will be to load the webpage using the driver we just downloaded. We will need to tell the WebDriver where the driver is located by setting the executable path on the Chrome class. Then we can request the webpage by using the get method of the driver.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    import os
    from selenium import webdriver

    driver = webdriver.Chrome(executable_path= os.path.abspath('')+'/chromedriver')
    url = &quot;http://www.sadesmith.com/examples/simple-scrapper/index.html&quot;
    driver.get(url)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we will need to tell the WebDriver to wait a few seconds before parsing the page. We will import the time module and use the sleep method. We will give the method the value of 3, thus the wait time will be 3 seconds.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    import os
    from selenium import webdriver
    import time

    driver = webdriver.Chrome(executable_path= os.path.abspath('')+'/chromedriver')
    url = &quot;http://www.sadesmith.com/examples/simple-scrapper/index.html&quot;
    driver.get(url)
    time.sleep(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are ready to parse but before we do that let’s import beautiful soup.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    from bs4 import BeautifulSoup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can grab the content we need to parse. With the WebDriver loading the page, we can now view content loaded client-side. WebDriver’s page_source will return the source code of the DOM as a string. We can use BeautifulSoup to parse the HTML string, with its HTML parser.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    page_html = driver.page_source
    bsoup = BeautifulSoup(page_html, 'html.parser')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the BeautifulSoup object, we can then use the library to its full extent. If you are unfamiliar with BeautifulSoup you can find more documentation here about how to parse: https://www.crummy.com/software/BeautifulSoup/bs4/doc/. For our example, we know that all the names on the Warrior’s rosters are in an unordered list, in list item tag with the class name of “name”. So we can grab all list item elements with the class attribute name then loop through them print them out.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    player_names = bsoup.findAll('li', attrs={'class': 'name'});
    for player_name in player_names:
      print player_name.text
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;wrapping-it-all-up&quot;&gt;Wrapping it all up&lt;/h2&gt;

&lt;p&gt;We are done with the simple scrapper and your file should now look this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    import os
    from selenium import webdriver
    import time
    from bs4 import BeautifulSoup

    driver = webdriver.Chrome(executable_path= os.path.abspath('')+'/chromedriver')
    driver.get(url)
    time.sleep(3)

    page_html = driver.page_source
    bsoup = BeautifulSoup(page_html, 'html.parser')

    player_names = bsoup.findAll('li', attrs={'class': 'name'});
    for player_name in player_names:
        print player_name.text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s run the project in the terminal! Make sure your current working directory is “simple-scrapper” and run the script.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-terminal&quot;&gt;    python scrapper.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If everything went well, you should see the names of the players on the 2018 Golden State Warriors printed in your terminal screen. We have accomplished our objective! Via our simple web scraper script, we were able to scrape HTML that was generated on the client-side using Selenium and Python.&lt;/p&gt;</content><author><name></name></author><category term="python" /><category term="selenium" /><category term="automation" /><category term="tutorial" /><category term="web scraping" /><summary type="html">Scraping data from websites often offers a way to automate tedious tasks and improve productivity. However, many people run into issues when the content of a website is generated on the client side as opposed to the server-side.</summary></entry><entry><title type="html">Overview of the HTTP/2 Protocol</title><link href="http://localhost:4000/2018/06/02/blog/overview-of-the-http2-protocol/" rel="alternate" type="text/html" title="Overview of the HTTP/2 Protocol" /><published>2018-06-02T00:00:00-05:00</published><updated>2018-06-02T00:00:00-05:00</updated><id>http://localhost:4000/2018/06/02/blog/overview-of-the-http2-protocol</id><content type="html" xml:base="http://localhost:4000/2018/06/02/blog/overview-of-the-http2-protocol/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2015, the HyperText Transfer Protocol (HTTP) underwent a major revision when HTTP/2 became officially standardized. With the protocol’s last major update being in 1999 (HTTP/1.1), it was a much needed upgrade. The previous version of the protocol, HTTP/1.1, proved to posses numerous limitations for most modern websites. The average HTTP requests per page have risen dramatically overtime as well as page size. Simply put, HTTP/1.1 was developed to handle websites of its time and not of today. These limitations later led developers to use HTTP/1.1 in ways it was not meant to be used, causing developers to create workarounds which developed into best practices. For example, with HTTP/1.1 a client only has a limited number of TCP connections (6 connections) to a server, and each request requires one of those connections. With limited connections a request must wait for another to finish before being processed (HOL Blocking), resulting in a bottleneck for websites retrieving resources, slowing down performance. To solve this problem, file concatenation, image sprites, domain sharding, using gzip, and resource inlining became workarounds and best practices for this particular issue of HOL Blocking. These solutions proved to be hacks, not really solving the issue at hand, which was really at the protocol level.&lt;/p&gt;

&lt;p&gt;In 2009, Google recognized these issues for their own operations of transporting web content and began working on an internal project called SPDY. SPDY was a network protocol that offered solutions to many of HTTP/1.1’s shortcomings, ofadfering addresings issues o web security and improved page load latency. This protocol would later become the foundation of HTTP/2.&lt;/p&gt;

&lt;p&gt;Fast forward to 2018, SPDY is deprecated and HTTP/2 is the new current version of HTTP with adoption increasing. HTTP/2 resolves the problems with HTTP/1.1 at the protocol level and provides faster performance for a website. The new protocol is backwards compatible and has the same semantics as HTTP/2, but improves how data is sent and what data is sent. The core features of HTTP/2 are multiplexing, binary framing, header compression, prioritization, and server push. With the update, many of the best practices developers utilize are invalidated and are detrimental to performance. These new core features of HTTP/2 will require new best practices for developers to utilize it benefits.&lt;/p&gt;

&lt;h2 id=&quot;multiplexingbinary-framingstream-prioritization&quot;&gt;Multiplexing/Binary Framing/Stream Prioritization&lt;/h2&gt;

&lt;p&gt;First, let’s talk about multiplexing and binary framing. HTTP/2 now technically utilizes a single TLS encrypted connection. While HTTP/1.1 uses a request/response pair, HTTP/2 uses a logical stream. These streams are put into binary frames, encoded in a binary format, and then put on the connection. Streams share the connection and the bandwidth of the connection (multiplexing). If one stream is in a blocking state, this does not prevent other streams from accessing the connection and it’s resources. Additionally, a bonus, the client can specify dependencies and weight for each stream, this is called stream prioritization. But now with HTTP/2 multiple requests for data can be sent in parallel - multiple requests at the same time. HOL blocking is no longer a problem, where it was in HTTP/1.1. Now because requests are cheap, best practices for receiving more out of a request and reduce the number of requests, such as concatenation and spriting, no longer apply.&lt;/p&gt;

&lt;h2 id=&quot;header-compression&quot;&gt;Header Compression&lt;/h2&gt;

&lt;p&gt;Another major issue with HTTP/1.1 pertained to the metadata sent in header. As a result of HTTP being stateless, redundant, long, and static information was sent across each request. One such examples is cookies and user agent data. In addition, the data sent in the headers was in human readable text. These two things combined created the issue of wasted of bandwidth. Developers tried to work around this issue by using gzip to compress the data that goes over the connection. But gzip only compresses the data and not the headers itself. Now with HTTP/2, headers can be compressed because they are seperated from the data. HTTP/2 compresses headers using a new technology specifically for HTTP header compression called HPACK. However with the use of HPACK, compression and decompression works on the connection, meaning more connections are counterproductive. Best practices such as domain sharding and use of multiple origins for cdns are no longer considered best practice for HTTP/2.&lt;/p&gt;

&lt;h2 id=&quot;server-push&quot;&gt;Server Push&lt;/h2&gt;

&lt;p&gt;Lastly, let’s discuss the new push feature. With HTTP/2, the server can respond to a request that hasn’t been sent yet. With this new feature, the server can send any asset or file to the client it thinks it will need. For example, the server can send the css and javascript file because it understands that the client will need it as well after an intial request from the browser is sent for the index.html. Once the client is ready to recieve those files pushed, it will be ready for the client waiting in the cache. Adding further additional optimization of speed.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;To take advantage of the new protocol, there are some new best practices to follow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reduce DNS Lookups&lt;/li&gt;
  &lt;li&gt;Reuse TCP connections, connections are expensive&lt;/li&gt;
  &lt;li&gt;Use a CDN but reduce the number of origins&lt;/li&gt;
  &lt;li&gt;Minimize number of HTTP redirects&lt;/li&gt;
  &lt;li&gt;Eliminate unnecessary metadata&lt;/li&gt;
  &lt;li&gt;Compress assets during transfer (let gzip compress on the server)&lt;/li&gt;
  &lt;li&gt;Cache resources on the client&lt;/li&gt;
  &lt;li&gt;Eliminate unnecessary resources. Fetch what you need. Bytes are expensive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As stated previously, since 2015, the world wide web has seen increasing adoption of the new protocol. Companies such as Cloudflare, major hosting providers like Bluehost, and most majors browsers now offer support for the technology. If your site is using a TLS/SSL connection, and your servers from your hosting company support HTTP/2, it is likely that you are already using HTTP/2! Every connection starts out as HTP/1.1 then upgrades to HTTP/2 if all requirements are met. These three requirements must be met to use HTTP/2: (1) Server must support HTTP/2, (2) your application must use an HTTPS connection via TLS/SSL, and (3) the browser of a user must support HTTP/2. If at least one of the requirements are not met, then HTTP/1.1 is used. Developers should be looking to take advantage of the new upgrades provided by HTTP/2 as it’s adoption continues to grow. It’s essentially free optimization for your website!&lt;/p&gt;</content><author><name></name></author><category term="http" /><category term="http/2" /><category term="http/1.1" /><category term="spdy" /><summary type="html">In 2015, the HyperText Transfer Protocol (HTTP) underwent a major revision when HTTP/2 became officially standardized. With the protocol's last major update being in 1999 (HTTP/1.1), it was a much needed upgrade. The previous version of the protocol, HTTP/1.1, proved to posses numerous limitations for most modern websites.</summary></entry></feed>